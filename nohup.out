base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.0_epoch_0_iter_5000
model_size: 3b
gpu_id: '0'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  4:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/17/2024, 01:06:45 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.0_epoch_0_iter_5000
04/17/2024, 01:06:47 Model loaded in 2.7 seconds.
04/17/2024, 01:06:47 Start decoding ... 
04/17/2024, 01:06:47 Model output to: mnt/data_10t/flan_t5_distill/outputs/gsm8k_test_0.0.5.0_epoch_0_iter_5000.txt
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:14<03:51, 14.48s/it] 12%|█▏        | 2/17 [00:23<02:47, 11.15s/it] 18%|█▊        | 3/17 [00:31<02:19,  9.99s/it] 24%|██▎       | 4/17 [00:40<02:02,  9.42s/it] 29%|██▉       | 5/17 [00:49<01:51,  9.26s/it] 35%|███▌      | 6/17 [00:58<01:40,  9.16s/it] 41%|████      | 7/17 [01:06<01:28,  8.88s/it] 47%|████▋     | 8/17 [01:15<01:18,  8.73s/it] 53%|█████▎    | 9/17 [01:23<01:09,  8.68s/it] 59%|█████▉    | 10/17 [01:32<01:00,  8.70s/it] 65%|██████▍   | 11/17 [01:40<00:51,  8.63s/it] 71%|███████   | 12/17 [01:49<00:43,  8.65s/it] 76%|███████▋  | 13/17 [01:58<00:34,  8.74s/it] 82%|████████▏ | 14/17 [02:07<00:26,  8.84s/it] 88%|████████▊ | 15/17 [02:16<00:17,  8.95s/it] 94%|█████████▍| 16/17 [02:25<00:08,  8.93s/it]100%|██████████| 17/17 [02:33<00:00,  8.55s/it]100%|██████████| 17/17 [02:33<00:00,  9.02s/it]
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.0_epoch_0_iter_5000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 123, in main
    _, _, _, _ = parse_pred_ans(output_path)
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/src/utils.py", line 445, in parse_pred_ans
    with open(filename) as fd: lines = fd.readlines()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/encodings/ascii.py", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_5_iter_40000
model_size: 3b
gpu_id: '0'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  4:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/17/2024, 01:42:08 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_5_iter_40000
04/17/2024, 01:42:24 Model loaded in 16.9 seconds.
04/17/2024, 01:42:24 Start decoding ... 
04/17/2024, 01:42:24 Model output to: mnt/data_10t/flan_t5_distill/outputs/gsm8k_test_0.0.2.2_epoch_5_iter_40000.txt
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:10<02:44, 10.31s/it] 12%|█▏        | 2/17 [00:18<02:17,  9.15s/it] 18%|█▊        | 3/17 [00:27<02:03,  8.80s/it] 24%|██▎       | 4/17 [00:34<01:50,  8.47s/it] 29%|██▉       | 5/17 [00:43<01:40,  8.40s/it] 35%|███▌      | 6/17 [00:52<01:33,  8.52s/it] 41%|████      | 7/17 [01:00<01:24,  8.41s/it] 47%|████▋     | 8/17 [01:08<01:15,  8.37s/it] 53%|█████▎    | 9/17 [01:16<01:07,  8.38s/it] 59%|█████▉    | 10/17 [01:24<00:57,  8.27s/it] 65%|██████▍   | 11/17 [01:33<00:49,  8.27s/it] 71%|███████   | 12/17 [01:41<00:41,  8.30s/it] 76%|███████▋  | 13/17 [01:50<00:33,  8.35s/it] 82%|████████▏ | 14/17 [01:58<00:25,  8.49s/it] 88%|████████▊ | 15/17 [02:07<00:17,  8.51s/it] 94%|█████████▍| 16/17 [02:15<00:08,  8.49s/it]100%|██████████| 17/17 [02:22<00:00,  7.92s/it]100%|██████████| 17/17 [02:22<00:00,  8.38s/it]
num_q 1319 correct 199 ratio 0.1509 skipped 0
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 79, in main
    model.to('cuda:' + str(args.gpu_id))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2576, in to
    return super().to(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
model_size: 3b
gpu_id: '5'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  5:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  6:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/17/2024, 23:56:49 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 79, in main
    model.to('cuda:' + str(args.gpu_id))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2576, in to
    return super().to(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1133, in to
    device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
RuntimeError: Invalid device string: 'cuda:0,1'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
model_size: 3b
gpu_id: 0,1
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 00:22:41 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
model_size: 3b
gpu_id: '0'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 00:31:06 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
04/18/2024, 00:31:11 Model loaded in 4.8 seconds.
04/18/2024, 00:31:11 Start decoding ... 
04/18/2024, 00:31:11 Model output to: mnt/data_10t/flan_t5_distill/outputs/gsm8k_test_0.0.5.1_epoch_0_iter_1000.txt
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:17<04:45, 17.83s/it] 12%|█▏        | 2/17 [00:30<03:41, 14.75s/it] 18%|█▊        | 3/17 [00:43<03:15, 13.98s/it] 24%|██▎       | 4/17 [00:55<02:54, 13.39s/it] 29%|██▉       | 5/17 [01:08<02:35, 12.93s/it] 35%|███▌      | 6/17 [01:20<02:19, 12.65s/it] 41%|████      | 7/17 [01:31<02:01, 12.13s/it] 47%|████▋     | 8/17 [01:43<01:48, 12.09s/it] 53%|█████▎    | 9/17 [01:54<01:35, 11.97s/it] 59%|█████▉    | 10/17 [02:06<01:21, 11.70s/it] 65%|██████▍   | 11/17 [02:16<01:07, 11.33s/it] 71%|███████   | 12/17 [02:26<00:54, 10.92s/it] 71%|███████   | 12/17 [02:37<01:05, 13.11s/it]
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 127, in <module>
    main()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 120, in main
    ans_ = tokenizer.decode(ans_).replace('<pad>', '').strip()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3825, in decode
    return self._decode(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 1024, in _decode
    sub_texts.append(self.convert_tokens_to_string(current_sub_text))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py", line 441, in convert_tokens_to_string
    if token in self.all_special_tokens:
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1352, in all_special_tokens
    all_toks = [str(s) for s in self.all_special_tokens_extended]
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1338, in all_special_tokens_extended
    tokens_to_add = [token for token in value if str(token) not in seen]
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1338, in <listcomp>
    tokens_to_add = [token for token in value if str(token) not in seen]
KeyboardInterrupt
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
model_size: 3b
gpu_id: '0'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 00:54:58 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_1000
04/18/2024, 00:55:04 Model loaded in 6.3 seconds.
04/18/2024, 00:55:04 Start decoding ... 
04/18/2024, 00:55:04 Model output to: mnt/data_10t/flan_t5_distill/outputs/gsm8k_test_0.0.5.1_epoch_0_iter_1000.txt
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:15<04:08, 15.54s/it] 12%|█▏        | 2/17 [00:27<03:18, 13.23s/it] 18%|█▊        | 3/17 [00:38<02:50, 12.18s/it] 24%|██▎       | 4/17 [00:49<02:34, 11.90s/it] 29%|██▉       | 5/17 [01:01<02:23, 11.95s/it] 35%|███▌      | 6/17 [01:13<02:11, 11.94s/it] 41%|████      | 7/17 [01:24<01:57, 11.75s/it] 47%|████▋     | 8/17 [01:36<01:45, 11.72s/it] 53%|█████▎    | 9/17 [01:47<01:33, 11.63s/it] 59%|█████▉    | 10/17 [01:59<01:21, 11.69s/it] 65%|██████▍   | 11/17 [02:10<01:09, 11.50s/it] 71%|███████   | 12/17 [02:22<00:58, 11.64s/it] 76%|███████▋  | 13/17 [02:33<00:45, 11.44s/it] 76%|███████▋  | 13/17 [02:38<00:48, 12.20s/it]
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 127, in <module>
    main()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 114, in main
    outputs = model.generate(inputs['input_ids'].to(model.device), 
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/generation/utils.py", line 1527, in generate
    result = self._greedy_search(
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/generation/utils.py", line 2471, in _greedy_search
    unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py", line 137, in __call__
    is_done = is_done | criteria(input_ids, scores, **kwargs)
KeyboardInterrupt
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_5000
model_size: 3b
gpu_id: '0'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 00:58:01 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_0_iter_5000
04/18/2024, 00:58:20 Model loaded in 19.3 seconds.
04/18/2024, 00:58:20 Start decoding ... 
04/18/2024, 00:58:20 Model output to: mnt/data_10t/flan_t5_distill/outputs/gsm8k_test_0.0.5.1_epoch_0_iter_5000.txt
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:15<04:00, 15.03s/it] 12%|█▏        | 2/17 [00:27<03:20, 13.39s/it] 18%|█▊        | 3/17 [00:38<02:56, 12.63s/it] 24%|██▎       | 4/17 [00:48<02:30, 11.55s/it] 29%|██▉       | 5/17 [00:57<02:05, 10.49s/it] 35%|███▌      | 6/17 [01:06<01:48,  9.88s/it] 41%|████      | 7/17 [01:14<01:32,  9.25s/it] 47%|████▋     | 8/17 [01:25<01:29,  9.95s/it] 53%|█████▎    | 9/17 [01:33<01:15,  9.38s/it] 59%|█████▉    | 10/17 [01:45<01:10, 10.02s/it] 65%|██████▍   | 11/17 [01:57<01:03, 10.66s/it] 71%|███████   | 12/17 [02:13<01:02, 12.41s/it] 76%|███████▋  | 13/17 [02:29<00:53, 13.34s/it] 82%|████████▏ | 14/17 [02:44<00:41, 13.95s/it] 88%|████████▊ | 15/17 [02:56<00:26, 13.45s/it] 94%|█████████▍| 16/17 [03:08<00:12, 13.00s/it]100%|██████████| 17/17 [03:19<00:00, 12.29s/it]100%|██████████| 17/17 [03:19<00:00, 11.73s/it]
num_q 1319 correct 49 ratio 0.0371 skipped 0
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 79, in main
    model.to('cuda:' + str(args.gpu_id))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2576, in to
    return super().to(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
    return self._apply(convert)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
    param_applied = fn(param)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
model_size: 3b
gpu_id: '4'
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 01:12:46 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 79, in main
    model.to('cuda:' + str(args.gpu_id))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2576, in to
    return super().to(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1133, in to
    device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
RuntimeError: Invalid device string: 'cuda:4,5'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
model_size: 3b
gpu_id: 4,5
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 01:18:27 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 79, in main
    model.to('cuda:' + str(args.gpu_id))
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2576, in to
    return super().to(*args, **kwargs)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1133, in to
    device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
RuntimeError: Invalid device string: 'cuda:4,5'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
model_size: 3b
gpu_id: 4,5
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  4:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  5:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 01:23:36 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.2.2_epoch_0_iter_20000
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 75, in main
    model = T5ForConditionalGeneration.from_pretrained(args.base_model)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3310, in from_pretrained
    with safe_open(resolved_archive_file, framework="pt") as f:
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000
model_size: 3b
gpu_id: 4,5
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  4:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  5:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 21:22:40 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000
Error executing job with overrides: ['base_model=mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000', 'output_path=mnt/data_10t/flan_t5_distill/outputs/', 'batch_size_fixed=80', 'test_data=gsm8k_test']
Traceback (most recent call last):
  File "/home/group2024-rqn/FlanT5-CoT-Specialization/test_distill.py", line 75, in main
    model = T5ForConditionalGeneration.from_pretrained(args.base_model)
  File "/home/group2024-rqn/miniconda3/envs/rqn/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3310, in from_pretrained
    with safe_open(resolved_archive_file, framework="pt") as f:
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
base_model: mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000
model_size: 3b
gpu_id: 0,1
batch_size_fixed: 80
batch_size: 40
prompt_path: lib_prompt/prompt_simple_4_cases.txt
output_path: mnt/data_10t/flan_t5_distill/outputs/
test_data: gsm8k_test
device_map:
  0:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  1:
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11

04/18/2024, 21:23:56 Loading the model from mnt/data_10t/flan_t5_distill/checkpoints/0.0.5.1_epoch_1_iter_60000
